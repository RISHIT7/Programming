{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Subtask 1\n",
    "\n",
    "## File Types\n",
    "\n",
    "#### 1. CSV\n",
    "    a. A CSV (Comma-Separated Values) file is a plain text file that stores tabular data\n",
    "    (numbers and text) in plain text form, with each line representing a row of data and\n",
    "    each field (or column) separated by a delimiter, commonly a comma.\n",
    "    b. CSV files are not designed for efficient random access. If we need to access data at a\n",
    "    specific location, we may need to read the file sequentially until we reach the\n",
    "    desired position.\n",
    "#### 2. TXT\n",
    "    a. Regarding \"txt\" files, it typically implies plain text files without a specific structure or\n",
    "       standardized format.\n",
    "#### 3. Pickle\n",
    "    a. Regarding \"pickle\" files, the pickle module allows us to serialize and deserialize\n",
    "       Python objects, storing them in a “binary” format.\n",
    "#### 4. Parquet\n",
    "    a. Parquet is a columnar storage file format commonly used for big data processing\n",
    "       frameworks like Apache Spark and Apache Hive.\n",
    "#### 5. HDF\n",
    "    a. HDF5 files have a hierarchical structure, allowing efficient complex data organization.\n",
    "       This structure enables quick access to specific datasets or groups within the file.\n",
    "#### 6. Feather\n",
    "    a. Like Parquet, Feather uses a columnar storage format, where data from the same\n",
    "       column is stored together. This facilitates fast and efficient access to specific columns,\n",
    "       making it suitable for analytical queries.\n",
    "#### 7. JSON\n",
    "    a. JSON (JavaScript Object Notation) is a lightweight data-interchange format that is\n",
    "       easy for humans to read and write and for machines to parse and generate.\n",
    "#### 8. AVRO\n",
    "    a. Apache Avro is a binary serialization format developed within the Apache Hadoop\n",
    "       project. It is compact, fast, and designed for efficient data serialization.\n",
    "\n",
    "## Read and Write time Analysis\n",
    "\n",
    "#### 1. CSV\n",
    "\n",
    "    a. Reading and writing operations in CSV files are generally efficient because the format\n",
    "    is simple, and data can be sequentially processed line by line.\n",
    "    b. Reading a CSV file involves parsing each line and splitting it based on the linear\n",
    "    operation of the delimiter.\n",
    "    c. Writing to a CSV file is also straightforward, as you can append lines to the file one at\n",
    "    a time.\n",
    "\n",
    "#### 2. TXT\n",
    "\n",
    "    a. Reading and writing operations in plain text files are generally efficient. Operations\n",
    "    involve the sequential processing of lines.\n",
    "\n",
    "    b. Reading a text file typically involves reading each line one at a time, making it a\n",
    "    linear operation.\n",
    "    c. Writing to a text file is straightforward, as data can be appended or modified\n",
    "    sequentially.\n",
    "\n",
    "#### 3. Pickle\n",
    "\n",
    "    a. Pickling (serialization) and unpickling (deserialization) operations can be time-\n",
    "    efficient for complex data structures and objects.\n",
    "    b. Pickle files are binary, and the serialization process captures the internal structure of\n",
    "    Python objects, including their state.\n",
    "\n",
    "#### 4. Parquet\n",
    "\n",
    "    a. Reading and processing specific columns is faster than row-based storage formats\n",
    "    like CSV or plain text.\n",
    "\n",
    "#### 5. HDF\n",
    "\n",
    "    a. HDF5 supports chunking, a mechanism where data is stored in fixed-size chunks. This\n",
    "    can improve read and write performance, especially when working with large\n",
    "    datasets, as it allows for selective access to specific parts of the data without reading\n",
    "    the entire file.\n",
    "\n",
    "#### 6. Feather\n",
    "\n",
    "    a. Feather is designed to be a lightweight and fast serialization format. The binary\n",
    "    format allows for rapid serialization and deserialization of data, contributing to\n",
    "    efficient read and write operations.\n",
    "\n",
    "#### 7. JSON\n",
    "\n",
    "    a. Parsing JSON is a linear operation, making it efficient for reading and writing small to\n",
    "    moderately sized datasets.\n",
    "\n",
    "#### 8. AVRO\n",
    "\n",
    "    a. Avro uses binary encoding, contributing to faster serialization and deserialization\n",
    "    than text-based formats like JSON. This is especially beneficial for large datasets and\n",
    "    high-throughput scenarios.\n",
    "\n",
    "## Space and Storage Analysis\n",
    "\n",
    "#### 1. CSV\n",
    "\n",
    "    a. CSV files are relatively space-efficient because they store data in a simple text format\n",
    "    without additional overhead.\n",
    "    b. Compared to binary formats, CSV files may occupy more space due to the human-\n",
    "    readable nature of the format and the inclusion of text-based delimiters and quotes.\n",
    "\n",
    "#### 2. TXT\n",
    "\n",
    "    a. Like CSV files, plain text files are not optimized for efficient random access. If random\n",
    "    access is required, reading the file sequentially may be necessary.\n",
    "    b. Plain text files are generally space-efficient because they store data in a simple,\n",
    "    human-readable format without additional formatting overhead.\n",
    "    c. However, the lack of a standardized structure means that the space efficiency can\n",
    "    vary based on how the data is organized within the file.\n",
    "\n",
    "#### 3. Pickle\n",
    "\n",
    "    a. pickle files are more suitable for random access than plain text or CSV files. Since the\n",
    "    file contains a serialized representation of objects, you can selectively load specific\n",
    "    objects without reading the entire file.\n",
    "\n",
    "    b. Pickle files can be more space-efficient than plain text files because the binary\n",
    "    format is more compact.\n",
    "    c. The serialized format includes information about the object's structure, allowing for\n",
    "    efficient representation of complex data types.\n",
    "\n",
    "#### 4. Parquet\n",
    "\n",
    "    a. Parquet files store data in a columnar format, meaning values from the same column\n",
    "    are stored together. This can lead to significant performance improvements,\n",
    "    especially for analytics and queries that select specific columns.\n",
    "    b. The columnar storage format of Parquet files contributes to space efficiency by\n",
    "    reducing the storage required for duplicate values within a column.\n",
    "\n",
    "#### 5. HDF\n",
    "\n",
    "    a. The chunking mechanism in HDF5 improves time efficiency and can contribute to\n",
    "    space efficiency. It allows for efficient storage of large datasets by breaking them into\n",
    "    manageable chunks.\n",
    "    b. HDF5 allows the creation of virtual datasets, defined as references to data stored\n",
    "    elsewhere in the file. This feature supports efficient storage and organization of data\n",
    "    without duplication.\n",
    "\n",
    "#### 6. Feather\n",
    "\n",
    "    a. Like Parquet, Feather uses a columnar storage format, where data from the same\n",
    "    column is stored together. This facilitates fast and efficient access to specific columns,\n",
    "    making it suitable for analytical queries.\n",
    "    b. The columnar storage format of Feather contributes to space efficiency by reducing\n",
    "    redundant storage of similar values within a column.\n",
    "\n",
    "#### 7. JSON\n",
    "\n",
    "    a. JSON is relatively quick to serialize (convert objects to JSON format) and deserialize\n",
    "    (convert JSON format to objects) due to its simple and text-based structure.\n",
    "    b. JSON is a text-based format, so it may not be as space-efficient as binary formats.\n",
    "    Text-based formats tend to have more overhead due to the inclusion of characters\n",
    "    like curly braces, colons, and quotes.\n",
    "\n",
    "#### 8. AVRO\n",
    "\n",
    "    a.Avro's binary format is designed to be compact, resulting in smaller file sizes\n",
    "    compared to text-based formats. The compactness is achieved through efficient\n",
    "    encoding of data types and minimal metadata overhead.\n",
    "\n",
    "\n",
    "## Graph\n",
    "\n",
    "![](./media/SBIN.png)\n",
    "\n",
    "### RISHIT JAKHARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jugaad_data.nse import stock_df\n",
    "import seaborn as sns\n",
    "import ta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(macd_line, signal_line):\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    sns.set_style('dark')\n",
    "    sns.set_theme('paper')\n",
    "    sns.lineplot(x = macd_line.index, y = macd_line, label = 'macd_line (SBIN)')\n",
    "    sns.lineplot(x = signal_line.index, y = signal_line, label = 'signal_line (SBIN)')\n",
    "    diff = signal_line-macd_line\n",
    "    # sns.barplot(x = diff.index, y = diff, palette= ['red', 'green'], hue)\n",
    "    plt.bar(diff.index, diff, color=['green' if val > 0 else 'red' for val in diff], width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(test_data, data):\n",
    "    df = pd.DataFrame()\n",
    "    df['P. CLOSE'] = data['PREV. CLOSE']\n",
    "    df['P. OPEN'] = data['OPEN'].shift(1) \n",
    "    df['P. VWAP'] = data['VWAP'].shift(1)\n",
    "    df['P. LOW'] = data['LOW'].shift(1) \n",
    "    df['P. HIGH'] = data['HIGH'].shift(1)\n",
    "    df['P. NO OF TRADES'] = data['NO OF TRADES'].shift(1)\n",
    "    df['OPEN'] = data['OPEN']\n",
    "    df['ONES'] = 1\n",
    "    df = df.drop(index=0)\n",
    "    matrix = np.array(df)\n",
    "\n",
    "    x_matrix = np.roll(matrix, shift=1, axis=1)\n",
    "    y_matrix = data['CLOSE'].drop(index=0)\n",
    "    \n",
    "    x_transpose = x_matrix.transpose()\n",
    "    x_transpose_x = np.array(x_transpose.dot(x_matrix))\n",
    "    inverse = np.linalg.inv(x_transpose_x)\n",
    "    x_transpose_y = np.array(x_transpose.dot(y_matrix))\n",
    "    params = np.array(inverse.dot(x_transpose_y))\n",
    "    \n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs(df1, df2):\n",
    "    spread = pd.DataFrame(df1['CLOSE'] - df2['CLOSE'])\n",
    "    sq_spread = spread*spread\n",
    "    roll = spread.rolling(window = 20).mean()\n",
    "    sumOfSquares = sq_spread.rolling(window = 20).sum()\n",
    "    \n",
    "    variance = (sumOfSquares/20 - (roll*roll))\n",
    "    sd = variance**0.5\n",
    "    z_score = np.array((spread-roll)/sd)\n",
    "    stock = 0\n",
    "    cashflow = 0\n",
    "    for i, score in enumerate(z_score):\n",
    "        if score > 2 and stock > -5:\n",
    "            stock -= 1\n",
    "            cashflow += df1['CLOSE'][i]\n",
    "            cashflow -= df2['CLOSE'][i]\n",
    "        if score < -2 and stock < 5:\n",
    "            stock += 1\n",
    "            cashflow -= df1['CLOSE'][i]\n",
    "            cashflow += df2['CLOSE'][i]\n",
    "        print(cashflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          DATE   CLOSE    HIGH     LOW  PREV. CLOSE    VWAP  NO OF TRADES  \\\n",
      "0   05/12/2022  617.30  618.00  607.55       607.55  614.05        210728   \n",
      "1   06/12/2022  608.95  619.80  607.80       617.30  613.29        190699   \n",
      "2   07/12/2022  607.05  612.90  604.50       608.95  607.96        113416   \n",
      "3   08/12/2022  611.65  613.80  607.15       607.05  611.14        146277   \n",
      "4   09/12/2022  616.50  618.00  609.10       611.65  613.84        169991   \n",
      "5   12/12/2022  613.05  618.70  611.00       616.50  613.74        135955   \n",
      "6   13/12/2022  616.75  617.40  612.50       613.05  615.45        162670   \n",
      "7   14/12/2022  625.50  626.75  617.50       616.75  623.52        187174   \n",
      "8   15/12/2022  615.95  629.55  614.30       625.50  621.92        148853   \n",
      "9   16/12/2022  603.35  615.60  602.10       615.95  607.83        160875   \n",
      "10  19/12/2022  604.45  609.50  603.00       603.35  605.78        149442   \n",
      "11  20/12/2022  604.45  606.50  599.55       604.45  602.65        128701   \n",
      "12  21/12/2022  593.40  609.15  589.60       604.45  599.66        145768   \n",
      "13  22/12/2022  593.40  599.00  587.55       593.40  592.49        136417   \n",
      "14  23/12/2022  574.00  590.70  571.50       593.40  579.55        226901   \n",
      "15  26/12/2022  597.10  601.70  570.70       574.00  589.64        210003   \n",
      "16  27/12/2022  601.90  603.10  593.30       597.10  598.21        206219   \n",
      "17  28/12/2022  601.05  607.00  598.55       601.90  602.94        143945   \n",
      "18  29/12/2022  611.80  618.00  597.15       601.05  608.16        168187   \n",
      "19  30/12/2022  613.70  620.55  611.35       611.80  615.43        158347   \n",
      "20  02/01/2023  612.20  619.20  611.20       613.70  614.42        125418   \n",
      "\n",
      "      OPEN  \n",
      "0   608.50  \n",
      "1   614.75  \n",
      "2   610.85  \n",
      "3   609.75  \n",
      "4   614.40  \n",
      "5   614.80  \n",
      "6   613.50  \n",
      "7   618.50  \n",
      "8   625.55  \n",
      "9   612.00  \n",
      "10  604.00  \n",
      "11  603.00  \n",
      "12  605.90  \n",
      "13  597.00  \n",
      "14  590.00  \n",
      "15  574.00  \n",
      "16  600.40  \n",
      "17  600.90  \n",
      "18  600.00  \n",
      "19  615.25  \n",
      "20  614.95  \n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-2873.7000000000003\n",
      "-5737.65\n",
      "-8573.55\n",
      "-8573.55\n",
      "-8573.55\n",
      "-10795.05\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-13149.699999999999\n",
      "-11738.5\n",
      "-11738.5\n",
      "-11738.5\n",
      "-11738.5\n",
      "-11738.5\n",
      "-11738.5\n",
      "-11738.5\n",
      "-9989.55\n",
      "-7937.45\n",
      "-6044.549999999999\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-4088.3499999999995\n",
      "-5767.299999999999\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-7505.8499999999985\n",
      "-5650.249999999998\n",
      "-5650.249999999998\n",
      "-5650.249999999998\n",
      "-3778.249999999998\n",
      "-3778.249999999998\n",
      "-3778.249999999998\n",
      "-1836.1499999999983\n",
      "132.35000000000218\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "2114.950000000002\n",
      "4183.000000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "6312.550000000002\n",
      "4497.200000000003\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "2712.9000000000024\n",
      "957.2000000000021\n",
      "-743.5999999999979\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-2399.7999999999975\n",
      "-540.7499999999973\n",
      "1287.2500000000027\n",
      "1287.2500000000027\n",
      "1287.2500000000027\n",
      "3223.7500000000027\n",
      "5574.850000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n",
      "7850.350000000002\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ generation of data frame --------------------------------------------\n",
    "def generate_dataframe(symbol, today, lastday, train_data):\n",
    "    # formatting the date\n",
    "    to_day = int(today[:2])\n",
    "    to_month = int(today[3:5])\n",
    "    to_year = int(today[6:10])\n",
    "    \n",
    "    la_day = int(lastday[:2])\n",
    "    la_month = int(lastday[3:5])\n",
    "    la_year = int(lastday[6:10])\n",
    "    \n",
    "    df = pd.DataFrame(stock_df(symbol=symbol, from_date=date(to_year, to_month, to_day), to_date=date(la_year, la_month, la_day), series=\"EQ\"))\n",
    "    df = df[[ \"DATE\", \"CLOSE\", \"HIGH\", \"LOW\", \"PREV. CLOSE\", \"VWAP\", \"NO OF TRADES\", \"OPEN\"]]\n",
    "    df = df.iloc[::-1]\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%d-%m-%Y')\n",
    "    df['DATE'] = df['DATE'].dt.strftime('%d/%m/%Y')\n",
    "    if (train_data == \"0\"):\n",
    "        df.to_csv(\"Stocks/\"+symbol+\".csv\", index=False)\n",
    "    else:\n",
    "        df.to_csv(\"Stocks/\"+symbol+\"_train.csv\", index = False)\n",
    "\n",
    "# ----------------------------------------------- pickle --------------------------------------------------------\n",
    "def write_pickle(DATA, symbol):\n",
    "    pd.to_pickle(DATA, symbol + \".pkl\")\n",
    "\n",
    "# -------------------------------------------------------- MAIN -----------------------------------------------------------------------\n",
    "def main():\n",
    "    arguments = [\"SBIN\", \"ADANIENT\"]\n",
    "    lastday = \"01/01/2024\"\n",
    "    today = \"05/12/2022\"\n",
    "    train_data = \"0\"\n",
    "    for i, argument in enumerate(arguments):\n",
    "        generate_dataframe(argument, today, lastday, train_data)\n",
    "    #write_pickle(DATA, \"Stocks/\" + argument)\n",
    "    \n",
    "    df1 = pd.read_csv('Stocks/SBIN.csv')\n",
    "    df2 = pd.read_csv('Stocks/ADANIENT.csv')\n",
    "    pairs(df1, df2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['A', 'B'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10016\\3554134031.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rishi\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rishi\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m# we require a list, but not a 'str'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rishi\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"agg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mis_groupby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rishi\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m                 \u001b[0mcols_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Column(s) {cols_sorted} do not exist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0maggregator_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column(s) ['A', 'B'] do not exist\""
     ]
    }
   ],
   "source": [
    "df.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
